# PIPELINE DEFINITION
# Name: mlops-ref-monitoring-pipeline
# Inputs:
#    alias: str
#    dataset_id: str
#    dataset_region: str
#    feature_table_id: str
#    model_name: str
#    monitoring_metrics_history_table: str
#    project: str
#    region: str
#    required_columns: list
#    source_gcs_dir: str
#    source_table_id: str
#    target_column: str
#    top_k_feat_prep: int
#    validation_script_path: str
#    validation_sql_params: dict
#    write_disposition_bq_load: str
#    write_disposition_feature_load: str
components:
  comp-bigquery-query-job:
    executorLabel: exec-bigquery-query-job
    inputDefinitions:
      parameters:
        encryption_spec_key_name:
          defaultValue: ''
          description: Describes the Cloud KMS encryption key that will be used to
            protect destination BigQuery table. The BigQuery Service Account associated
            with your project requires access to this encryption key. If encryption_spec_key_name
            are both specified in here and in job_configuration_query, the value in
            here will override the other one.
          isOptional: true
          parameterType: STRING
        job_configuration_query:
          defaultValue: {}
          description: A json formatted string describing the rest of the job configuration.  For
            more details, see https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationQuery
          isOptional: true
          parameterType: STRUCT
        labels:
          defaultValue: {}
          description: 'The labels associated with this job. You can use these to
            organize and group your jobs. Label keys and values can be no longer than
            63 characters, can only containlowercase letters, numeric characters,
            underscores and dashes. International characters are allowed. Label values
            are optional. Label keys must start with a letter and each label in the
            list must have a different key.

            Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.'
          isOptional: true
          parameterType: STRUCT
        location:
          defaultValue: us-central1
          description: Location for creating the BigQuery job. If not set, default
            to `US` multi-region.  For more details, see https://cloud.google.com/bigquery/docs/locations#specifying_your_location
          isOptional: true
          parameterType: STRING
        project:
          defaultValue: '{{$.pipeline_google_cloud_project_id}}'
          description: Project to run the BigQuery query job. Defaults to the project
            in which the PipelineJob is run.
          isOptional: true
          parameterType: STRING
        query:
          defaultValue: ''
          description: SQL query text to execute. Only standard SQL is supported.  If
            query are both specified in here and in job_configuration_query, the value
            in here will override the other one.
          isOptional: true
          parameterType: STRING
        query_parameters:
          defaultValue: []
          description: jobs.query parameters for standard SQL queries.  If query_parameters
            are both specified in here and in job_configuration_query, the value in
            here will override the other one.
          isOptional: true
          parameterType: LIST
    outputDefinitions:
      artifacts:
        destination_table:
          artifactType:
            schemaTitle: google.BQTable
            schemaVersion: 0.0.1
          description: Describes the table where the query results should be stored.
            This property must be set for large results that exceed the maximum response
            size. For queries that produce anonymous (cached) results, this field
            will be populated by BigQuery.
      parameters:
        gcp_resources:
          description: Serialized gcp_resources proto tracking the BigQuery job. For
            more details, see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
          parameterType: STRING
  comp-bq-load:
    executorLabel: exec-bq-load
    inputDefinitions:
      parameters:
        dataset_id:
          parameterType: STRING
        gcs_source_dir:
          parameterType: STRING
        is_monitoring:
          parameterType: BOOLEAN
        project:
          parameterType: STRING
        region:
          defaultValue: US
          isOptional: true
          parameterType: STRING
        table_id:
          parameterType: STRING
        write_disposition:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        raw_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        load_result:
          parameterType: BOOLEAN
  comp-calculate-metrics:
    executorLabel: exec-calculate-metrics
    inputDefinitions:
      artifacts:
        model_input:
          artifactType:
            schemaTitle: google.UnmanagedContainerModel
            schemaVersion: 0.0.1
      parameters:
        dataset_id:
          parameterType: STRING
        feature_table_id:
          parameterType: STRING
        metrics_history_table:
          parameterType: STRING
        project:
          parameterType: STRING
        region:
          parameterType: STRING
        target_column:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        metrics_output:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-check-model-exists:
    executorLabel: exec-check-model-exists
    inputDefinitions:
      parameters:
        location:
          parameterType: STRING
        model_name:
          parameterType: STRING
        project:
          parameterType: STRING
    outputDefinitions:
      parameters:
        model_id:
          parameterType: STRING
  comp-check-validation-result:
    executorLabel: exec-check-validation-result
    inputDefinitions:
      parameters:
        dataset_id:
          parameterType: STRING
        project_id:
          parameterType: STRING
        region:
          defaultValue: US
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      artifacts:
        result_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        validation_result:
          parameterType: BOOLEAN
  comp-condition-2:
    dag:
      tasks:
        bigquery-query-job:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-bigquery-query-job
          dependentTasks:
          - fetch-validation-script
          inputs:
            parameters:
              location:
                componentInputParameter: pipelinechannel--dataset_region
              project:
                componentInputParameter: pipelinechannel--project
              query:
                taskOutputParameter:
                  outputParameterKey: output_script
                  producerTask: fetch-validation-script
          taskInfo:
            name: bigquery-query-job
        check-validation-result:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-check-validation-result
          dependentTasks:
          - bigquery-query-job
          inputs:
            parameters:
              dataset_id:
                componentInputParameter: pipelinechannel--dataset_id
              project_id:
                componentInputParameter: pipelinechannel--project
              region:
                componentInputParameter: pipelinechannel--dataset_region
          taskInfo:
            name: check-validation-result
        condition-3:
          componentRef:
            name: comp-condition-3
          dependentTasks:
          - check-validation-result
          inputs:
            parameters:
              pipelinechannel--alias:
                componentInputParameter: pipelinechannel--alias
              pipelinechannel--bq-load-load_result:
                componentInputParameter: pipelinechannel--bq-load-load_result
              pipelinechannel--check-validation-result-validation_result:
                taskOutputParameter:
                  outputParameterKey: validation_result
                  producerTask: check-validation-result
              pipelinechannel--dataset_id:
                componentInputParameter: pipelinechannel--dataset_id
              pipelinechannel--dataset_region:
                componentInputParameter: pipelinechannel--dataset_region
              pipelinechannel--feature_table_id:
                componentInputParameter: pipelinechannel--feature_table_id
              pipelinechannel--model_name:
                componentInputParameter: pipelinechannel--model_name
              pipelinechannel--monitoring_metrics_history_table:
                componentInputParameter: pipelinechannel--monitoring_metrics_history_table
              pipelinechannel--project:
                componentInputParameter: pipelinechannel--project
              pipelinechannel--region:
                componentInputParameter: pipelinechannel--region
              pipelinechannel--required_columns:
                componentInputParameter: pipelinechannel--required_columns
              pipelinechannel--source_table_id:
                componentInputParameter: pipelinechannel--source_table_id
              pipelinechannel--target_column:
                componentInputParameter: pipelinechannel--target_column
              pipelinechannel--top_k_feat_prep:
                componentInputParameter: pipelinechannel--top_k_feat_prep
              pipelinechannel--write_disposition_feature_load:
                componentInputParameter: pipelinechannel--write_disposition_feature_load
          taskInfo:
            name: condition-3
          triggerPolicy:
            condition: inputs.parameter_values['pipelinechannel--check-validation-result-validation_result']
              == true
        fetch-validation-script:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-fetch-validation-script
          inputs:
            parameters:
              sql_params:
                componentInputParameter: pipelinechannel--validation_sql_params
              validation_script_path:
                componentInputParameter: pipelinechannel--validation_script_path
          taskInfo:
            name: fetch-validation-script
    inputDefinitions:
      parameters:
        pipelinechannel--alias:
          parameterType: STRING
        pipelinechannel--bq-load-load_result:
          parameterType: BOOLEAN
        pipelinechannel--dataset_id:
          parameterType: STRING
        pipelinechannel--dataset_region:
          parameterType: STRING
        pipelinechannel--feature_table_id:
          parameterType: STRING
        pipelinechannel--model_name:
          parameterType: STRING
        pipelinechannel--monitoring_metrics_history_table:
          parameterType: STRING
        pipelinechannel--project:
          parameterType: STRING
        pipelinechannel--region:
          parameterType: STRING
        pipelinechannel--required_columns:
          parameterType: LIST
        pipelinechannel--source_table_id:
          parameterType: STRING
        pipelinechannel--target_column:
          parameterType: STRING
        pipelinechannel--top_k_feat_prep:
          parameterType: NUMBER_INTEGER
        pipelinechannel--validation_script_path:
          parameterType: STRING
        pipelinechannel--validation_sql_params:
          parameterType: STRUCT
        pipelinechannel--write_disposition_feature_load:
          parameterType: STRING
  comp-condition-3:
    dag:
      tasks:
        check-model-exists:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-check-model-exists
          dependentTasks:
          - prepare-features
          inputs:
            parameters:
              location:
                componentInputParameter: pipelinechannel--region
              model_name:
                componentInputParameter: pipelinechannel--model_name
              project:
                componentInputParameter: pipelinechannel--project
          taskInfo:
            name: check-model-exists
        condition-4:
          componentRef:
            name: comp-condition-4
          dependentTasks:
          - check-model-exists
          inputs:
            parameters:
              pipelinechannel--alias:
                componentInputParameter: pipelinechannel--alias
              pipelinechannel--bq-load-load_result:
                componentInputParameter: pipelinechannel--bq-load-load_result
              pipelinechannel--check-model-exists-model_id:
                taskOutputParameter:
                  outputParameterKey: model_id
                  producerTask: check-model-exists
              pipelinechannel--check-validation-result-validation_result:
                componentInputParameter: pipelinechannel--check-validation-result-validation_result
              pipelinechannel--dataset_id:
                componentInputParameter: pipelinechannel--dataset_id
              pipelinechannel--dataset_region:
                componentInputParameter: pipelinechannel--dataset_region
              pipelinechannel--feature_table_id:
                componentInputParameter: pipelinechannel--feature_table_id
              pipelinechannel--monitoring_metrics_history_table:
                componentInputParameter: pipelinechannel--monitoring_metrics_history_table
              pipelinechannel--project:
                componentInputParameter: pipelinechannel--project
              pipelinechannel--region:
                componentInputParameter: pipelinechannel--region
              pipelinechannel--target_column:
                componentInputParameter: pipelinechannel--target_column
          taskInfo:
            name: condition-4
          triggerPolicy:
            condition: inputs.parameter_values['pipelinechannel--check-model-exists-model_id']
              != ''
        prepare-features:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-prepare-features
          inputs:
            parameters:
              dataset_id:
                componentInputParameter: pipelinechannel--dataset_id
              feature_bq_table:
                componentInputParameter: pipelinechannel--feature_table_id
              is_monitoring:
                runtimeValue:
                  constant: true
              project:
                componentInputParameter: pipelinechannel--project
              raw_data_bq_table:
                componentInputParameter: pipelinechannel--source_table_id
              region:
                componentInputParameter: pipelinechannel--dataset_region
              required_columns:
                componentInputParameter: pipelinechannel--required_columns
              top_k:
                componentInputParameter: pipelinechannel--top_k_feat_prep
              write_disposition:
                componentInputParameter: pipelinechannel--write_disposition_feature_load
          taskInfo:
            name: prepare-features
    inputDefinitions:
      parameters:
        pipelinechannel--alias:
          parameterType: STRING
        pipelinechannel--bq-load-load_result:
          parameterType: BOOLEAN
        pipelinechannel--check-validation-result-validation_result:
          parameterType: BOOLEAN
        pipelinechannel--dataset_id:
          parameterType: STRING
        pipelinechannel--dataset_region:
          parameterType: STRING
        pipelinechannel--feature_table_id:
          parameterType: STRING
        pipelinechannel--model_name:
          parameterType: STRING
        pipelinechannel--monitoring_metrics_history_table:
          parameterType: STRING
        pipelinechannel--project:
          parameterType: STRING
        pipelinechannel--region:
          parameterType: STRING
        pipelinechannel--required_columns:
          parameterType: LIST
        pipelinechannel--source_table_id:
          parameterType: STRING
        pipelinechannel--target_column:
          parameterType: STRING
        pipelinechannel--top_k_feat_prep:
          parameterType: NUMBER_INTEGER
        pipelinechannel--write_disposition_feature_load:
          parameterType: STRING
  comp-condition-4:
    dag:
      tasks:
        calculate-metrics:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-calculate-metrics
          dependentTasks:
          - load-model
          inputs:
            artifacts:
              model_input:
                taskOutputArtifact:
                  outputArtifactKey: model_output
                  producerTask: load-model
            parameters:
              dataset_id:
                componentInputParameter: pipelinechannel--dataset_id
              feature_table_id:
                componentInputParameter: pipelinechannel--feature_table_id
              metrics_history_table:
                componentInputParameter: pipelinechannel--monitoring_metrics_history_table
              project:
                componentInputParameter: pipelinechannel--project
              region:
                componentInputParameter: pipelinechannel--dataset_region
              target_column:
                componentInputParameter: pipelinechannel--target_column
          taskInfo:
            name: calculate-metrics
        load-model:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-load-model
          inputs:
            parameters:
              alias:
                componentInputParameter: pipelinechannel--alias
              location:
                componentInputParameter: pipelinechannel--region
              model_id:
                componentInputParameter: pipelinechannel--check-model-exists-model_id
              project_id:
                componentInputParameter: pipelinechannel--project
          taskInfo:
            name: load-model
    inputDefinitions:
      parameters:
        pipelinechannel--alias:
          parameterType: STRING
        pipelinechannel--bq-load-load_result:
          parameterType: BOOLEAN
        pipelinechannel--check-model-exists-model_id:
          parameterType: STRING
        pipelinechannel--check-validation-result-validation_result:
          parameterType: BOOLEAN
        pipelinechannel--dataset_id:
          parameterType: STRING
        pipelinechannel--dataset_region:
          parameterType: STRING
        pipelinechannel--feature_table_id:
          parameterType: STRING
        pipelinechannel--monitoring_metrics_history_table:
          parameterType: STRING
        pipelinechannel--project:
          parameterType: STRING
        pipelinechannel--region:
          parameterType: STRING
        pipelinechannel--target_column:
          parameterType: STRING
  comp-exit-handler-1:
    dag:
      tasks:
        bq-load:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-bq-load
          inputs:
            parameters:
              dataset_id:
                componentInputParameter: pipelinechannel--dataset_id
              gcs_source_dir:
                componentInputParameter: pipelinechannel--source_gcs_dir
              is_monitoring:
                runtimeValue:
                  constant: true
              project:
                componentInputParameter: pipelinechannel--project
              region:
                componentInputParameter: pipelinechannel--dataset_region
              table_id:
                componentInputParameter: pipelinechannel--source_table_id
              write_disposition:
                componentInputParameter: pipelinechannel--write_disposition_bq_load
          taskInfo:
            name: bq-load
        condition-2:
          componentRef:
            name: comp-condition-2
          dependentTasks:
          - bq-load
          inputs:
            parameters:
              pipelinechannel--alias:
                componentInputParameter: pipelinechannel--alias
              pipelinechannel--bq-load-load_result:
                taskOutputParameter:
                  outputParameterKey: load_result
                  producerTask: bq-load
              pipelinechannel--dataset_id:
                componentInputParameter: pipelinechannel--dataset_id
              pipelinechannel--dataset_region:
                componentInputParameter: pipelinechannel--dataset_region
              pipelinechannel--feature_table_id:
                componentInputParameter: pipelinechannel--feature_table_id
              pipelinechannel--model_name:
                componentInputParameter: pipelinechannel--model_name
              pipelinechannel--monitoring_metrics_history_table:
                componentInputParameter: pipelinechannel--monitoring_metrics_history_table
              pipelinechannel--project:
                componentInputParameter: pipelinechannel--project
              pipelinechannel--region:
                componentInputParameter: pipelinechannel--region
              pipelinechannel--required_columns:
                componentInputParameter: pipelinechannel--required_columns
              pipelinechannel--source_table_id:
                componentInputParameter: pipelinechannel--source_table_id
              pipelinechannel--target_column:
                componentInputParameter: pipelinechannel--target_column
              pipelinechannel--top_k_feat_prep:
                componentInputParameter: pipelinechannel--top_k_feat_prep
              pipelinechannel--validation_script_path:
                componentInputParameter: pipelinechannel--validation_script_path
              pipelinechannel--validation_sql_params:
                componentInputParameter: pipelinechannel--validation_sql_params
              pipelinechannel--write_disposition_feature_load:
                componentInputParameter: pipelinechannel--write_disposition_feature_load
          taskInfo:
            name: condition-2
          triggerPolicy:
            condition: inputs.parameter_values['pipelinechannel--bq-load-load_result']
              == true
    inputDefinitions:
      parameters:
        pipelinechannel--alias:
          parameterType: STRING
        pipelinechannel--dataset_id:
          parameterType: STRING
        pipelinechannel--dataset_region:
          parameterType: STRING
        pipelinechannel--feature_table_id:
          parameterType: STRING
        pipelinechannel--model_name:
          parameterType: STRING
        pipelinechannel--monitoring_metrics_history_table:
          parameterType: STRING
        pipelinechannel--project:
          parameterType: STRING
        pipelinechannel--region:
          parameterType: STRING
        pipelinechannel--required_columns:
          parameterType: LIST
        pipelinechannel--source_gcs_dir:
          parameterType: STRING
        pipelinechannel--source_table_id:
          parameterType: STRING
        pipelinechannel--target_column:
          parameterType: STRING
        pipelinechannel--top_k_feat_prep:
          parameterType: NUMBER_INTEGER
        pipelinechannel--validation_script_path:
          parameterType: STRING
        pipelinechannel--validation_sql_params:
          parameterType: STRUCT
        pipelinechannel--write_disposition_bq_load:
          parameterType: STRING
        pipelinechannel--write_disposition_feature_load:
          parameterType: STRING
  comp-fetch-validation-script:
    executorLabel: exec-fetch-validation-script
    inputDefinitions:
      parameters:
        sql_params:
          parameterType: STRUCT
        validation_script_path:
          parameterType: STRING
    outputDefinitions:
      parameters:
        output_script:
          parameterType: STRING
  comp-load-model:
    executorLabel: exec-load-model
    inputDefinitions:
      parameters:
        alias:
          parameterType: STRING
        location:
          parameterType: STRING
        model_id:
          parameterType: STRING
        project_id:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        model_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-prepare-features:
    executorLabel: exec-prepare-features
    inputDefinitions:
      parameters:
        dataset_id:
          parameterType: STRING
        feature_bq_table:
          parameterType: STRING
        is_monitoring:
          parameterType: BOOLEAN
        project:
          parameterType: STRING
        raw_data_bq_table:
          parameterType: STRING
        region:
          parameterType: STRING
        required_columns:
          parameterType: LIST
        top_k:
          defaultValue: 5.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        write_disposition:
          defaultValue: WRITE_TRUNCATE
          isOptional: true
          parameterType: STRING
  comp-vertex-pipelines-notification-email:
    executorLabel: exec-vertex-pipelines-notification-email
    inputDefinitions:
      parameters:
        pipeline_task_final_status:
          isOptional: true
          parameterType: TASK_FINAL_STATUS
        recipients:
          description: A list of email addresses to send a notification to.
          parameterType: LIST
defaultPipelineRoot: gs://gs://mlops_ref/monitoring_pipeline/
deploymentSpec:
  executors:
    exec-bigquery-query-job:
      container:
        args:
        - --type
        - BigqueryQueryJob
        - --project
        - '{{$.inputs.parameters[''project'']}}'
        - --location
        - '{{$.inputs.parameters[''location'']}}'
        - --payload
        - '{"Concat": ["{", "\"configuration\": {", "\"query\": ", "{{$.inputs.parameters[''job_configuration_query'']}}",
          ", \"labels\": ", "{{$.inputs.parameters[''labels'']}}", "}", "}"]}'
        - --job_configuration_query_override
        - '{"Concat": ["{", "\"query\": \"", "{{$.inputs.parameters[''query'']}}",
          "\"", ", \"query_parameters\": ", "{{$.inputs.parameters[''query_parameters'']}}",
          ", \"destination_encryption_configuration\": {", "\"kmsKeyName\": \"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
          "\"}", "}"]}'
        - --gcp_resources
        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
        - --executor_input
        - '{{$}}'
        command:
        - python3
        - -u
        - -m
        - google_cloud_pipeline_components.container.v1.bigquery.query_job.launcher
        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.22.0
    exec-bq-load:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - bq_load
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'google-cloud-bigquery'\
          \ 'google-cloud-storage'  &&  python3 -m pip install --quiet --no-warn-script-location\
          \ 'kfp==2.14.6' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"\
          3.9\"' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef bq_load(\n    project: str,\n    gcs_source_dir: str,\n    dataset_id:\
          \ str,\n    table_id: str,\n    write_disposition: str,\n    raw_data: Output[Dataset],\n\
          \    is_monitoring: bool,\n    region: str = \"US\",\n)-> NamedTuple(\"\
          Outputs\", [(\"load_result\", bool)]):\n\n    from google.cloud import bigquery\n\
          \    from google.cloud import storage\n    from google.api_core.exceptions\
          \ import NotFound\n    from datetime import datetime, timedelta, timezone\n\
          \n    client = bigquery.Client(project=project, location=region)\n    storage_client\
          \ = storage.Client(project=project)\n\n    # 1. Clean the base URI\n   \
          \ if not gcs_source_dir.startswith(\"gs://\"):\n        raise ValueError(\"\
          gcs_source_dir must start with 'gs://'\")\n\n    # Ensure base path ends\
          \ with / for easy concatenation\n    base_uri = gcs_source_dir if gcs_source_dir.endswith(\"\
          /\") else gcs_source_dir + \"/\"\n\n    # 2. Determine target file path\
          \ based on mode\n    if is_monitoring:\n        # Monitoring looks for:\
          \ gs://.../YYYY-MM-DD/dataset.csv\n        yesterday_str = (datetime.now(timezone.utc)\
          \ - timedelta(days=1)).strftime('%Y-%m-%d')\n        target_gcs_uri = f\"\
          {base_uri}{yesterday_str}/dataset.csv\"\n    else:\n        # Training looks\
          \ for: gs://.../training_data/healthcare_dataset.csv\n        target_gcs_uri\
          \ = f\"{base_uri}training_data/healthcare_dataset.csv\"\n\n    print(f\"\
          Targeting URI: {target_gcs_uri}\")\n\n    # 3. Extract bucket and blob for\
          \ existence check\n    uri_parts = target_gcs_uri[5:].split(\"/\", 1)\n\
          \    bucket_name = uri_parts[0]\n    blob_name = uri_parts[1]\n\n    # 4.\
          \ Check if the file actually exists\n    bucket = storage_client.bucket(bucket_name)\n\
          \    blob = bucket.blob(blob_name)\n\n    if not blob.exists():\n      \
          \  if is_monitoring:\n            print(f\" Monitoring: No data found for\
          \ {yesterday_str} at {target_gcs_uri}\")\n            return (False,)\n\
          \        else:\n            raise FileNotFoundError(f\" Training: Required\
          \ file not found at {target_gcs_uri}\")\n\n    table_path = f\"{project}.{dataset_id}.{table_id}\"\
          \n    try:\n        client.get_table(table_path)\n    except NotFound:\n\
          \        print(f\"Table {table_path} not found. Creating with ingestion_time\
          \ column...\")\n        # We create an empty table first to define the special\
          \ column\n        # BigQuery will then use 'autodetect' to add the rest\
          \ of the columns during the load\n        query = f\"\"\"\n            CREATE\
          \ TABLE `{table_path}` (\n                ingestion_time TIMESTAMP DEFAULT\
          \ CURRENT_TIMESTAMP()\n            )\n        \"\"\"\n        client.query(query).result()\n\
          \n    job_config = bigquery.LoadJobConfig(\n        source_format=bigquery.SourceFormat.CSV,\n\
          \        skip_leading_rows=1,\n        write_disposition=write_disposition,\n\
          \        autodetect=True,\n        # 'allow_jagged_rows' allows the CSV\
          \ to have fewer columns than the BQ table\n        # which lets BQ fill\
          \ our 'ingestion_time' with the default value\n        allow_jagged_rows=True,\n\
          \        # Allow adding new columns from the CSV to the existing table schema\n\
          \        schema_update_options=[\n            bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION\n\
          \        ],\n    )\n    try:\n        load_job = client.load_table_from_uri(\n\
          \            target_gcs_uri,\n            table_path,\n            job_config=job_config,\n\
          \        )\n\n        load_job.result()  # Waits for the job to complete.\n\
          \        print(f\"Loaded {load_job.output_rows} rows into {dataset_id}:{table_id}.\"\
          )\n\n    except Exception as e:\n        raise RuntimeError(\n         \
          \   f\"BigQuery load failed for {target_gcs_uri} \"\n            f\"into\
          \ {project}.{dataset_id}.{table_id}: {e}\"\n        ) from e\n\n    raw_data.uri\
          \ = \"bq://\" + project + \".\" + dataset_id + \".\" + table_id\n    return\
          \ (True,)\n\n"
        image: python:3.10
    exec-calculate-metrics:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - calculate_metrics
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'pandas==2.0.3'\
          \ 'scikit-learn==1.3.2' 'google-cloud-bigquery' 'joblib==1.3.2' 'pyarrow'\
          \ 'db-dtypes' 'google-cloud-pipeline-components'  &&  python3 -m pip install\
          \ --quiet --no-warn-script-location 'kfp==2.14.6' '--no-deps' 'typing-extensions>=3.7.4,<5;\
          \ python_version<\"3.9\"' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\nfrom google_cloud_pipeline_components.types.artifact_types import UnmanagedContainerModel\n\
          \ndef calculate_metrics(\n    project: str,\n    region: str,\n    dataset_id:\
          \ str,\n    feature_table_id: str,\n    target_column: str,\n    model_input:\
          \ Input[UnmanagedContainerModel],\n    metrics_history_table: str,\n   \
          \ metrics_output: Output[Dataset]\n):\n    import pandas as pd\n    import\
          \ joblib\n    import os\n    from google.cloud import bigquery\n    from\
          \ sklearn.metrics import accuracy_score, precision_score, recall_score,\
          \ f1_score\n    from datetime import datetime\n\n    client = bigquery.Client(project=project,\
          \ location=region)\n\n    # 1. Fetch Today's Features (those created in\
          \ the previous step)\n    # We filter by CURRENT_DATE() to match our bq_load\
          \ logic\n    query = f\"\"\"\n        SELECT * FROM `{project}.{dataset_id}.{feature_table_id}`\
          \ \n        WHERE DATE(feature_timestamp) = CURRENT_DATE()\n    \"\"\"\n\
          \    features_df = client.query(query).to_dataframe()\n\n    if features_df.empty:\n\
          \        print(\"No new features found for today. Skipping metrics calculation.\"\
          )\n        return\n\n    # 2. Load the Model and Predict\n    model_dir\
          \ = model_input.path.rstrip(\"/\")\n    model_path = os.path.join(model_dir,\
          \ \"model.joblib\")\n    model = joblib.load(model_path)\n\n    # Drop metadata\
          \ columns before prediction\n    X = features_df.drop(columns=[target_column,\
          \ \"record_id\", \"feature_timestamp\"], errors='ignore')\n    y_true =\
          \ features_df[target_column]\n\n    # 4. Generate Predictions\n    y_pred\
          \ = model.predict(X)\n\n    # 5. Calculate Classification Metrics\n    #\
          \ We use 'weighted' average to handle potential class imbalance in healthcare\
          \ data\n    acc = accuracy_score(y_true, y_pred)\n    prec = precision_score(y_true,\
          \ y_pred, average='weighted', zero_division=0)\n    rec = recall_score(y_true,\
          \ y_pred, average='weighted', zero_division=0)\n    f1 = f1_score(y_true,\
          \ y_pred, average='weighted', zero_division=0)\n\n    # 6. Prepare Results\
          \ for History Table\n    results = {\n        \"run_ts\": datetime.now(),\n\
          \        \"model_name\": model_input.metadata.get(\"displayName\", \"unknown_model\"\
          ),\n        \"accuracy\": float(acc),\n        \"precision\": float(prec),\n\
          \        \"recall\": float(rec),\n        \"f1_score\": float(f1),\n   \
          \     \"sample_size\": len(features_df)\n    }\n\n    # 7. Append to BigQuery\
          \ Monitoring Table\n    # This acts as your \"Performance Ledger\"\n   \
          \ metrics_df = pd.DataFrame([results])\n    table_ref = f\"{project}.{dataset_id}.{metrics_history_table}\"\
          \n\n    job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_APPEND\"\
          )\n    try:\n        client.load_table_from_dataframe(metrics_df, table_ref,\
          \ job_config=job_config).result()\n        print(f\"Monitoring metrics logged\
          \ to {table_ref}\")\n    except Exception as e:\n        print(f\"Failed\
          \ to log metrics to BigQuery: {e}\")\n        raise RuntimeError(f\"Failed\
          \ to log metrics to BigQuery: {e}\")\n\n    # Output print for logs\n  \
          \  print(f\"Today's Performance -> F1: {f1:.4f}, Accuracy: {acc:.4f}\")\n\
          \    metrics_output.uri = f\"bq://{table_ref}\"\n\n"
        image: python:3.9
    exec-check-model-exists:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - check_model_exists
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform'\
          \  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.6'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef check_model_exists(model_name: str, project: str, location: str)\
          \ -> NamedTuple('Outputs',[('model_id', str)]):\n    from google.cloud import\
          \ aiplatform\n    import re\n\n    try:\n        aiplatform.init(project=project,\
          \ location=location)\n        filter_expression = f'displayName=\"{model_name}\"\
          '\n        models = aiplatform.Model.list(filter=filter_expression)\n\n\
          \        if len(models) == 0:\n            return (\"None\",)\n        else:\n\
          \            m = re.search(r\"models/([^/]+)$\", models[0].resource_name)\n\
          \            model_id = m.group(1)\n            print(model_id)\n      \
          \      return (model_id,)\n\n    except Exception:\n        print(f\"Model\
          \ {model_name} not found\")\n        return (\"None\",)\n\n"
        image: python:3.10
    exec-check-validation-result:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - check_validation_result
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'google-cloud-bigquery'\
          \  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.6'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef check_validation_result(\n    project_id: str,\n    dataset_id:\
          \ str, \n    result_output: Output[Artifact],\n    region: str = \"US\"\
          , \n) -> NamedTuple('Outputs', [('validation_result', bool)]):\n    import\
          \ json, os\n    from google.cloud import bigquery\n\n    os.makedirs(result_output.path,\
          \ exist_ok=True)\n    output_dir = os.path.join(result_output.path + '/validation_result.json')\n\
          \n    client = bigquery.Client(project=project_id, location=region)\n  \
          \  query = f\"\"\"\n     SELECT * from \n     `{project_id}.{dataset_id}.validation_results`\n\
          \     where validated_table = '{project_id}.{dataset_id}.healthcare_raw_data'\n\
          \     order by run_ts desc\n     limit 1;\n    \"\"\"\n    try:\n      \
          \  query_job = client.query(query)\n        results = query_job.result()\n\
          \        row = dict(next(results, None))\n        if row: \n           \
          \ with open(output_dir, 'w') as f:\n                json.dump(row, f, default=str)\n\
          \            return (row['passed'],)\n    except Exception as e:\n     \
          \   raise RuntimeError(f\"Failed to execute query\")\n\n"
        image: python:3.10
    exec-fetch-validation-script:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - fetch_validation_script
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'google-cloud-storage'\
          \  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.6'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef fetch_validation_script(\n    validation_script_path: str,\n\
          \    sql_params: dict,\n    # output_script: Output[Artifact]\n) -> NamedTuple('Outputs',\
          \ [('output_script', str)]):\n    \"\"\"\n    Downloads SQL script from\
          \ GCS and executes it in BigQuery.\n    Returns: job_id (string) of the\
          \ executed BigQuery job.\n    \"\"\"\n    from google.cloud import storage\n\
          \n    if not validation_script_path.startswith(\"gs://\"):\n        raise\
          \ ValueError(\"validation_script_path must start with gs://\")\n\n    _,\
          \ path_after = validation_script_path.split(\"gs://\", 1)\n    bucket_name,\
          \ _, blob_path = path_after.partition(\"/\")\n\n    storage_client = storage.Client()\n\
          \    blob = storage_client.bucket(bucket_name).blob(blob_path)\n    raw_script\
          \ = blob.download_as_text()\n    formatted_script =  raw_script.format(**sql_params)\n\
          \n    return (formatted_script,)  # Returning as a tuple to match NamedTuple\
          \ output\n\n"
        image: python:3.10
    exec-load-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - load_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform'\
          \ 'google-cloud-storage'  &&  python3 -m pip install --quiet --no-warn-script-location\
          \ 'kfp==2.14.6' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"\
          3.9\"' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef load_model(\n    project_id: str,\n    location: str,\n    model_id:\
          \ str,\n    alias: str,\n    model_output: Output[Artifact],\n):\n    '''\n\
          \    This component loads a machine learning model from Google Cloud AI\
          \ Platform Model Registry\n    and saves the model artifacts to a specified\
          \ output location.\n    '''\n    import os\n    from google.cloud import\
          \ aiplatform, storage\n\n    def fetch_model_artifacts_to_local(model_uri:\
          \ str, dest_dir: str):\n\n        client = storage.Client()\n        bucket_name,\
          \ prefix = model_uri.replace(\"gs://\", \"\").split(\"/\", 1)\n        os.makedirs(dest_dir,\
          \ exist_ok=True)\n\n        for blob in client.list_blobs(bucket_name, prefix=prefix):\n\
          \            if blob.name.endswith(\"/\"):\n                continue\n \
          \           filename = os.path.basename(blob.name)\n            if not filename:\n\
          \                continue\n            local_path = os.path.join(dest_dir,\
          \ filename)\n            try:\n                blob.download_to_filename(local_path)\n\
          \            except Exception as e:\n                data = blob.download_as_bytes()\n\
          \                with open(local_path, \"wb\") as f:\n                 \
          \   f.write(data)\n\n    aiplatform.init(project=project_id, location=location)\n\
          \    model = aiplatform.Model(\n        model_name=f\"projects/{project_id}/locations/{location}/models/{model_id}@{alias}\"\
          \n    )\n    uri = model.uri     \n    fetch_model_artifacts_to_local(uri,\
          \ model_output.path)\n\n"
        image: python:3.10
    exec-prepare-features:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - prepare_features
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'pandas' 'numpy'\
          \ 'google-cloud-bigquery' 'db-dtypes'  &&  python3 -m pip install --quiet\
          \ --no-warn-script-location 'kfp==2.14.6' '--no-deps' 'typing-extensions>=3.7.4,<5;\
          \ python_version<\"3.9\"' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef prepare_features(\n    project: str,\n    region: str,\n    raw_data_bq_table:\
          \ str,\n    feature_bq_table: str,\n    required_columns: list,\n    dataset_id:\
          \ str,\n    is_monitoring: bool,\n    top_k: int = 5,\n    write_disposition:\
          \ str = \"WRITE_TRUNCATE\",\n):\n    \"\"\"\n    Preprocess raw healthcare\
          \ data and write model-ready features to BigQuery.\n    \"\"\"\n\n    import\
          \ re\n    import pandas as pd\n    from google.cloud import bigquery\n \
          \   from google.api_core import exceptions as gcp_exceptions\n\n    # ------------------------------------------------------------------\n\
          \    # Helpers\n    # ------------------------------------------------------------------\n\
          \n    def normalize_whitespace(s):\n        if pd.isna(s):\n           \
          \ return s\n        return re.sub(r\"\\s+\", \" \", str(s)).strip()\n\n\
          \    def normalize_blood_type(s):\n        if pd.isna(s):\n            return\
          \ \"UNKNOWN\"\n        x = str(s).strip().upper().replace(\" \", \"\")\n\
          \        if re.fullmatch(r\"(A|B|AB|O)[+-]\", x):\n            return x\n\
          \        return \"UNKNOWN\"\n\n    def topk_map(series: pd.Series, k: int)\
          \ -> pd.Series:\n        series = series.fillna(\"UNKNOWN\")\n        top\
          \ = series.value_counts().nlargest(k).index\n        return series.apply(lambda\
          \ v: v if v in top else \"OTHER\")\n\n    def bucket_age(age):\n       \
          \ try:\n            a = float(age)\n        except Exception:\n        \
          \    return \"unknown\"\n        if a < 0:\n            return \"unknown\"\
          \n        if a <= 17:\n            return \"0-17\"\n        if a <= 34:\n\
          \            return \"18-34\"\n        if a <= 54:\n            return \"\
          35-54\"\n        if a <= 74:\n            return \"55-74\"\n        return\
          \ \"75+\"\n\n    # ------------------------------------------------------------------\n\
          \    # BigQuery Init\n    # ------------------------------------------------------------------\n\
          \n    client = bigquery.Client(project=project, location=region)\n\n   \
          \ raw_table_id = f\"{project}.{dataset_id}.{raw_data_bq_table}\"\n    dest_table_id\
          \ = f\"{project}.{dataset_id}.{feature_bq_table}\"\n\n    # ------------------------------------------------------------------\n\
          \    # Read BigQuery\n    # ------------------------------------------------------------------\n\
          \n    try:\n        if is_monitoring:\n            date_filter = \"WHERE\
          \ DATE(ingestion_time) = CURRENT_DATE()\"\n        else:\n            date_filter\
          \ = \"\"\n        if required_columns:\n            cols = \", \".join([f\"\
          `{c}`\" for c in required_columns])\n            query = f\"SELECT {cols}\
          \ FROM `{raw_table_id}` {date_filter}\"\n        else:\n            query\
          \ = f\"SELECT * FROM `{raw_table_id}` {date_filter}\"\n\n        df = (\n\
          \            client.query(query)\n            .result()\n            .to_dataframe(create_bqstorage_client=False)\n\
          \        )\n\n        print(f\"Read {len(df)} rows from {raw_table_id}\"\
          )\n\n    except gcp_exceptions.GoogleAPICallError as e:\n        raise RuntimeError(f\"\
          BigQuery read failed: {e}\") from e\n\n    if df.empty:\n        raise RuntimeError(f\"\
          Raw table {raw_table_id} is empty\")\n\n    # ------------------------------------------------------------------\n\
          \    # Rename Columns\n    # ------------------------------------------------------------------\n\
          \n    col_map = {\n        \"Name\": \"name\",\n        \"Age\": \"age\"\
          ,\n        \"Gender\": \"gender\",\n        \"Blood Type\": \"blood_type\"\
          ,\n        \"Medical Condition\": \"medical_condition\",\n        \"Date\
          \ of Admission\": \"date_of_admission\",\n        \"Doctor\": \"doctor\"\
          ,\n        \"Hospital\": \"hospital\",\n        \"Insurance Provider\":\
          \ \"insurance_provider\",\n        \"Billing Amount\": \"billing_amount\"\
          ,\n        \"Room Number\": \"room_number\",\n        \"Admission Type\"\
          : \"admission_type\",\n        \"Discharge Date\": \"discharge_date\",\n\
          \        \"Medication\": \"medication\",\n        \"Test Results\": \"test_results\"\
          ,\n        \"ingestion_time\": \"feature_timestamp\",\n    }\n\n    df =\
          \ df.rename(columns={k: v for k, v in col_map.items() if k in df.columns})\n\
          \n    # ------------------------------------------------------------------\n\
          \    # Deduplication\n    # ------------------------------------------------------------------\n\
          \    df = df.drop_duplicates(subset=[\"name\", \"age\"], keep=\"first\"\
          )\n    print(f\"Deduplicated data has {len(df)} rows\")\n\n    # ------------------------------------------------------------------\n\
          \    # Cleaning\n    # ------------------------------------------------------------------\n\
          \n    for c in df.select_dtypes(include=\"object\").columns:\n        df[c]\
          \ = df[c].str.strip()\n\n    df[\"age\"] = df[\"age\"].mask((df[\"age\"\
          ] < 0) | (df[\"age\"] > 120))\n    df[\"age_bucket\"] = df[\"age\"].apply(bucket_age)\n\
          \    df[\"age\"] = (df[\"age\"].astype(str).str.replace(\"\u2013\", \"-\"\
          , regex=False)  # EN DASH \u2192 ASCII\n)\n    df[\"blood_type\"] = df[\"\
          blood_type\"].apply(normalize_blood_type)\n\n    df[\"medical_condition\"\
          ] = (\n        df[\"medical_condition\"]\n        .astype(str)\n       \
          \ .apply(lambda s: normalize_whitespace(s).lower() if s != \"nan\" else\
          \ \"unknown\")\n    )\n\n    df[\"hospital\"] = df[\"hospital\"].astype(str).apply(normalize_whitespace)\n\
          \    df[\"hospital\"] = topk_map(df[\"hospital\"], top_k)\n\n    # ------------------------------------------------------------------\n\
          \    # DATE FIX (Timestamp vs date bug)\n    # ------------------------------------------------------------------\n\
          \n    df[\"date_of_admission\"] = pd.to_datetime(\n        df[\"date_of_admission\"\
          ], errors=\"coerce\"\n    )\n\n    df[\"discharge_date\"] = pd.to_datetime(\n\
          \        df[\"discharge_date\"], errors=\"coerce\"\n    )\n\n    df[\"admit_time_days\"\
          ] = (\n        df[\"discharge_date\"] - df[\"date_of_admission\"]\n    ).dt.days\n\
          \n    df[\"admit_time_days\"] = df[\"admit_time_days\"].mask(\n        (df[\"\
          admit_time_days\"] < 0) | (df[\"admit_time_days\"] > 365)\n    )\n\n   \
          \ median_los = df[\"admit_time_days\"].median(skipna=True)\n    df[\"admit_time_days\"\
          ] = df[\"admit_time_days\"].fillna(median_los)\n\n    df[\"admit_time_bucket\"\
          ] = pd.cut(\n        df[\"admit_time_days\"],\n        bins=[-1, 0, 2, 5,\
          \ 10, 30, 365],\n        labels=[\"0\", \"1\u20132\", \"3\u20135\", \"6\u2013\
          10\", \"11\u201330\", \"30+\"],\n    )\n    df[\"admit_time_bucket\"] =\
          \ (\n        df[\"admit_time_bucket\"]\n        .astype(str)\n        .str.replace(\"\
          \u2013\", \"-\", regex=False)  # EN DASH \u2192 ASCII\n    )\n\n    # ------------------------------------------------------------------\n\
          \    # Create a unique record_id \n    # ------------------------------------------------------------------\n\
          \    df[\"record_id\"] = (\n        df[\"name\"].astype(str).str.lower().str.replace(\"\
          \ \", \"\") + \n        \"_\" + \n        df[\"age\"].astype(str)\n    )\n\
          \    print(\"Number of records in df: \", len(df))\n    # ------------------------------------------------------------------\n\
          \    # Drop PII / Select Features\n    # ------------------------------------------------------------------\n\
          \n    allowlist = [\n        \"record_id\",        \n        \"age_bucket\"\
          ,\n        \"gender\",\n        \"blood_type\",\n        \"medical_condition\"\
          ,\n        \"admission_type\",\n        \"hospital\",\n        \"admit_time_bucket\"\
          ,\n        \"test_results\",\n        \"feature_timestamp\"\n    ]\n\n \
          \   features_df = df[[c for c in allowlist if c in df.columns]].copy()\n\
          \n    # ------------------------------------------------------------------\n\
          \    # FIX: Handle Categoricals SAFELY\n    # ------------------------------------------------------------------\n\
          \n    for c in features_df.columns:\n        if pd.api.types.is_categorical_dtype(features_df[c]):\n\
          \            if \"UNKNOWN\" not in features_df[c].cat.categories:\n    \
          \            features_df[c] = features_df[c].cat.add_categories([\"UNKNOWN\"\
          ])\n            features_df[c] = features_df[c].fillna(\"UNKNOWN\")\n  \
          \      else:\n            features_df[c] = features_df[c].fillna(\"UNKNOWN\"\
          ).astype(str)\n\n    # Optional hardening (safe for BigQuery ML)\n    features_df\
          \ = features_df.astype(str)\n    print(f\"Prepared features dataframe has\
          \ {features_df.shape[0]}\")\n    # ------------------------------------------------------------------\n\
          \    # Write to BigQuery\n    # ------------------------------------------------------------------\n\
          \n    try:\n        job_config = bigquery.LoadJobConfig(\n            write_disposition=(\n\
          \                bigquery.WriteDisposition.WRITE_APPEND\n              \
          \  if write_disposition == \"WRITE_APPEND\"\n                else bigquery.WriteDisposition.WRITE_TRUNCATE\n\
          \            )\n        )\n\n        load_job = client.load_table_from_dataframe(\n\
          \            features_df,\n            destination=dest_table_id,\n    \
          \        job_config=job_config,\n        )\n\n        load_job.result()\n\
          \n        print(\n            f\"Wrote {features_df.shape[0]} rows \"\n\
          \            f\"and {features_df.shape[1]} columns to {dest_table_id}\"\n\
          \        )\n\n    except gcp_exceptions.GoogleAPICallError as e:\n     \
          \   raise RuntimeError(f\"BigQuery write failed: {e}\") from e\n\n"
        image: python:3.10
    exec-vertex-pipelines-notification-email:
      container:
        args:
        - --type
        - VertexNotificationEmail
        - --payload
        - ''
        command:
        - python3
        - -u
        - -m
        - google_cloud_pipeline_components.container.v1.vertex_notification_email.executor
        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.22.0
pipelineInfo:
  name: mlops-ref-monitoring-pipeline
root:
  dag:
    tasks:
      exit-handler-1:
        componentRef:
          name: comp-exit-handler-1
        inputs:
          parameters:
            pipelinechannel--alias:
              componentInputParameter: alias
            pipelinechannel--dataset_id:
              componentInputParameter: dataset_id
            pipelinechannel--dataset_region:
              componentInputParameter: dataset_region
            pipelinechannel--feature_table_id:
              componentInputParameter: feature_table_id
            pipelinechannel--model_name:
              componentInputParameter: model_name
            pipelinechannel--monitoring_metrics_history_table:
              componentInputParameter: monitoring_metrics_history_table
            pipelinechannel--project:
              componentInputParameter: project
            pipelinechannel--region:
              componentInputParameter: region
            pipelinechannel--required_columns:
              componentInputParameter: required_columns
            pipelinechannel--source_gcs_dir:
              componentInputParameter: source_gcs_dir
            pipelinechannel--source_table_id:
              componentInputParameter: source_table_id
            pipelinechannel--target_column:
              componentInputParameter: target_column
            pipelinechannel--top_k_feat_prep:
              componentInputParameter: top_k_feat_prep
            pipelinechannel--validation_script_path:
              componentInputParameter: validation_script_path
            pipelinechannel--validation_sql_params:
              componentInputParameter: validation_sql_params
            pipelinechannel--write_disposition_bq_load:
              componentInputParameter: write_disposition_bq_load
            pipelinechannel--write_disposition_feature_load:
              componentInputParameter: write_disposition_feature_load
        taskInfo:
          name: exit-handler-1
      vertex-pipelines-notification-email:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-vertex-pipelines-notification-email
        dependentTasks:
        - exit-handler-1
        inputs:
          parameters:
            pipeline_task_final_status:
              taskFinalStatus:
                producerTask: exit-handler-1
            recipients:
              runtimeValue:
                constant:
                - sarthak.lohani@onixnet.com
        taskInfo:
          name: vertex-pipelines-notification-email
        triggerPolicy:
          strategy: ALL_UPSTREAM_TASKS_COMPLETED
  inputDefinitions:
    parameters:
      alias:
        parameterType: STRING
      dataset_id:
        parameterType: STRING
      dataset_region:
        parameterType: STRING
      feature_table_id:
        parameterType: STRING
      model_name:
        parameterType: STRING
      monitoring_metrics_history_table:
        parameterType: STRING
      project:
        parameterType: STRING
      region:
        parameterType: STRING
      required_columns:
        parameterType: LIST
      source_gcs_dir:
        parameterType: STRING
      source_table_id:
        parameterType: STRING
      target_column:
        parameterType: STRING
      top_k_feat_prep:
        parameterType: NUMBER_INTEGER
      validation_script_path:
        parameterType: STRING
      validation_sql_params:
        parameterType: STRUCT
      write_disposition_bq_load:
        parameterType: STRING
      write_disposition_feature_load:
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.14.6
